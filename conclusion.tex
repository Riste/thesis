\chapter{Conclusions and Discussion}\label{chap:conclusion}
\begin{quotation}
\noindent 
In this chapter we revisit the research questions outlined in Chap. \ref{chap:intro} and state the conclusions that follow from the research work presented in this thesis. We conclude with a final discussion and indicate possible research directions to extend or compliment the results described in the thesis.  
\end{quotation}

\section{Main Contributions}
As audio-visual archives become accessible to the wider internet audience, successful retrieval of archive items is an increasingly challenging task. One of the most important reason for this is the lack of adequate annotations. Users cannot find what they are looking for because annotations are either not
present or too expert-centric. Video tagging games are an attempt to alleviate this problem. Engaging the internet community to tag the videos addresses the issue of scarcity of annotations and yields tags that are community-centric. The focus of this thesis is on the role of game tags in audio-visual archives and their added value for video retrieval. In the typical audio-visual archive ecosystem the game tags will coexist with professionally curated annotations. To understand what do game tags bring to the table as opposed to the catalogers' annotations we study the differences between them along two dimensions. The first dimension is the \textit{terminology}: we perform a quantitative study to estimate the \textit{terminological gap} between taggers and the catalogers. The second dimension is the \textit{descriptive scope} i.e. which aspects of the video content are described and at what level of abstraction and granularity. We carry out a qualitative study in which we analyse the descriptive scope of the game tags. Making the videos more accessible to the wider internet audience is one of the main objectives of the game tags. To evaluate the efficiency of the game tags for video retrieval we design Cranfield-style experiments for two prominent scenarios that arise in practice: \textit{visual} and \textit{topical} search. Moreover we perform a qualitative analysis of samples of search results to establish the reasons that usually lead to true and false positives in both scenarios. The fast pace of \textit{Waisda?} usually results in players contributing mainly non-topical tags. To combat this effect in the topical search scenario, we characterize the quality of user tags as topical annotations and identify several tag features (filters) that serve as indicators whether game tags are useful for retrieval. The success of each of the filters is evaluated in Cranfield-style experiment.

The main contributions of the thesis are:
\begin{itemize}
\item A method to analyse quantitatively the overlap between user and professional terminology exploited in video annotation.
\item Qualitative analysis of the aspects of the video content which are described by the game tags i.e. \textit{what do game tags describe in a video and how}.
%\item A method of qualitative analysis of samples of user tags to establish the facets---\textit{who}, \textit{what}, \textit{where}, and \textit{when}--- of videos typically described by users and the level of specificity---\textit{abstract}, \textit{generic}, and \textit{specific}---of tags.
\item A method to design a dataset ---including fragments, queries, and relevance judgements --- for evaluating the added value of user tags for \textit{visual} and \textit{topical} video search.
\item Two evaluation datasets ---one for visual and one for topical search --- which contain fragments, user tags from \textit{Waisda?} collection, queries, and relevance judgements.
\item Quantitative evaluation of the added value of the game tags in terms of \textit{visual} and \textit{topical} video search.

\item A method for qualitative analysis/classification of the search results in the context of visual search. The aim of the analysis is to discover the types of tags that are generally responsible for false positives.

\item Qualitative analysis of the search results in the context of topical search. The aim of the analysis is to discover tagging practices that generally produce relevant topical tags.

\item Evaluation of tag-related measures for accessing the quality of the game tags as topical descriptors in the context of topical video search.

\item Development of extension module for \textit{Waisda?} which enables integration of the game in AV collection workflows. The module is part of the latest release of \textit{Waisda?} which is available online\footnote{The Waisda? source code and documentation is available from Github \url{http://github.com/beeldengeluid/waisda}}.
\end{itemize}

In this chapter we consider again the research questions stated in Chapter \ref{chap:intro} outlining the methods and results. The last section features discussion and directions for future research.

\section{Research Questions Revisited}
\subsection{What is the relationship between the game tags and professional annotations in terms of vocabulary and what they describe?}
In the typical audio-visual archive ecosystem the game tags coexist with professionally curated annotations.
Successful integration of the game tags into the AV collections' workflows requires a better understanding of the characteristics of the tags compared to the professional annotations. To this end, in Chapter \ref{chap:kcap}, we perform a quantitative and a qualitative study of the game tags in which we compare them to the professional annotations along two dimensions: (i) the vocabulary used i.e. the \textit{terminological gap} and (ii) the \textit{descriptive scope} i.e. which aspects of the video content are described and at what level of abstraction and granularity.

\subsubsection{Studying the Terminological Gap}
In order to study the terminological gap we devise and carry out a quantitative analysis of the game tags. 
Presently, \textit{Waisda?} has no built-in mechanisms that control the quality of the language used by the taggers. Therefore, the first goal of the analysis is to determine the fraction of the game tags that are meaningful words. For this purpose we exploit two vast background knowledge sources --- the general lexicon of the Dutch language Corneto and the Google search engine --- as semantic filters; whenever a tag is present in either of these two sources we take it as a signal that the tag is a meaningful word. Additionally, Corneto encodes part-of-speech information about the words which we exploit to quantify the types of words generally used by the taggers. Corneto also provides information about the various senses of the words which is used to detect polysemy. 
The results from the analysis show that the majority of the game tags are meaningful words in themselves. This analysis also reaffirmed some of the inherent limitations of the user tags compared to the controlled vocabularies; namely, subjectivity and ambiguity of the language. Using the part-of-speech information we detected that the majority of the tags found in Cornetto are nouns and verbs. This means that people generally focus on objects and actions when tagging the videos. Small fraction of the game tags found in Cornetto, around $15\%$, are adjectives. Generally these are subjective descriptive words, for example the most common adjective among the tags is \textit{lekker} (\textit{nice} in Dutch).
The general concern is that the overly subjective and imprecise tags can hinder searching. We come back to this in Section \ref{conclusion:folsonomy-flaw}.

Finally, to check whether there is a terminological gap between the game tags and professional annotations we matched all game tags to the domain thesaurus GTAA. GTAA is the sole vocabulary that is employed by the catalogers when annotating the same videos that are tagged in the game. We observe that only a small fraction of the unique game tags can be found in GTAA, around $8\%$. This showed that there is indeed a terminological gap in the way the gamers and catalogers describe the videos.

\subsubsection{The Descriptive Scope of Game Tags}
To get a better understanding of what aspects of the video content are described by the game tags we carried out a manual classification of a sample of game tags. We used a classification schema that combines the Panofsky-Shatford model and a classification framework built by Hollink at al \cite{laurapaper}. The results of the tag classification demonstrate that the game tags predominantly describe objects using generic language and rarely refer to topics of a scene or the entire video. This is in sharp contrast with the professional annotations which exclusively target the topic(s) of the entire video and (less frequently) of particular scenes. The general conclusion is that game tags and the professional annotations are complementary both in terms of the terminology and in terms of what aspects of the video content they describe. This makes the game tags a valuable asset for at least the following two reasons. First, they provide a more online community-centric view of the content which will undoubtedly improve the access for the wider internet audience. Second, game tags are fine-grained annotations that target scenes, events, and objects within scenes. At this level of granularity they can facilitate tasks like \textit{event detection}, \textit{instance search} etc. Moreover, this level of granularity generally is unattainable for the professional annotations due to the enormous effort and time required. Game tags can step in and fill the void.

\subsection{Can we improve video retrieval with the help of user-generated data?}\label{conclusion:search}
Video fragment retrieval is widely recognized as an application scenario of particular business importance in the AV archival world. We evaluate the effectiveness of the game tags for two retrieval tasks: (i) \textit{visual search}, retrieval of fragments that feature visual appearances of the concept of interest and (ii) \textit{topical search}, retrieval of fragments that are about a given topic.

\subsubsection{Game tags for visual search}
When evaluating the effectiveness of game tags for visual search in Chapter \ref{chap:ecir}, we are mainly interested in two research questions. First, can game tags on their own or in combination with other existing types of metadata improve search? And second, does using only verified (consensus) tags give better search performance than using all game tags? In addressing these questions we use Cranfield-style quantitative evaluation methodology. This methodology encompasses creation of an evaluation dataset that consists of a query set and relevance judgements. The query set is derived from real-life query logs. The relevance judgements are at the hearth of this methodology and reflect the fact that queries ought to be interpreted visually. We devised and carried out a crowdsourcing experiment to gather them by harnessing the efforts of the online \textit{Waisda?} community.

The results from the evaluation show that game tags are doing exceedingly well in visual search. They outperform  the other types of metadata in pairwise comparison. Moreover, combining the game tags with the other metadata types proved to be beneficial for visual search as well. In fact, including the game tags boosts the search performance for $33\%$. On the other hand, considering only verified tags results in worse search performance than considering all game tags. This is mainly caused by the fact that search based on all game tags yielded more relevant results and resulted in significantly higher recall. It seems the tag verification criterion is too conservative in a sense that it filters out tags that are in fact useful for search.

The overall conclusion is that game tags are well-suited for retrieving fragments that visually depict given object of interest. In Chapter \ref{chap:ecir} we establish that game tags usually describe events and objects within scenes so it is not surprising that they excel at visual search. We also establish that there is a difference in the focus and the scope of the game tags compared to the professional annotations; game tags being more fine-grained and professional annotations being more coarse-grained referring to the entire video and describing the prevalent topics. This property of the game tags makes them particularly appealing for business use-cases such as discovering/selling stock footage. Moreover, recent advancements in areas such as computer vision open an array of possibilities for hybrid approaches. This is discussed more in Section \ref{con:bigger-picture}.

\subsubsection{Game tags for topical search}
In Chapter \ref{chap:topicir-filter} we study the added value of game tags for topical search. To this end, we use once again the Cranfield-style quantitative evaluation methodology. Central to this methodology is the creation of relevance judgements. For the purpose of the study, we devise a procedure that automatized the process by exploiting the in-house catalog tags as the gold standard w.r.t. what topics are covered in the fragments. This allows us to consider much larger set of fragments since we are no longer dependent on human judges. 

The collected retrieval performance metrics show that when it comes to topical search the game tags leave much to be desired. While the retrieval recall of the game tags is relatively satisfactory the precision is rather poor. In order to understand the causes for the  poor retrieval performance we devise and carry out a qualitative analysis of samples of \textit{true} and \textit{false positives} retrieved using the game tags. The analysis encompasses manual classification where the tags in the samples are classified w.r.t. to the content component they refer to --- \textit{audio}, \textit{visual}, or \textit{both} --- and their descriptive scope (see Section \ref{topicir:qual-ana} for more details). The aim of the analysis is to investigate the relationship between the content component and the descriptive scope of the game tags on one side and whether they resulted in true of false positive on the other. 

The results of the analysis demonstrate that the overwhelming majority of the false positives are caused by game tags that do not pertain to the subject (\textit{semantics}) of the video, but refer to the more \textit{syntactic} aspects such as what is \textit{seen} or \textit{heard}. For example, many of these cases refer to general tags describing objects appearing in the foreground and the background of the scenery. It seems that the fast-pace of the game and the scoring mechanism which rewards tag consensus encourages the players to focus more on the immediate short-lived aspects of the video content rather than the more high-level topical aspects. After all it is easier to reach an agreement on prominent objects that draw the attention than on scene topics. The current game design of \textit{Waisda?} creates a bias that works against collecting topical tags which is evident from the difference in the results between visual and the topical search studies. 

What is more important, our analysis showed that a significant portion of the topics of the videos are indeed captured by the game tags. The topical tags are present albeit buried under the myriad of non-topical tags. Therefore, if the game tags are to be used for topical search, a preprocessing step is required that will identify and filter out the non-topical game tags. We address this issue in Chapter \ref{chap:topicir-filter} and we come back to it in the Section \ref{conclusion:filter}.

\subsubsection{Shortcoming of game tags for retrieval}\label{conclusion:folsonomy-flaw}
One of the main drawbacks of the user-generated tags compared to the controlled vocabularies is that tags are inherently imprecise. %\cite{Mathes04folksonomies,citeulike:468899}.
The players of \textit{Waisda?} are unconstrained when entering tags and this can lead to several limitations. There is no explicit meaning assigned to the tags which can result in \textit{ambiguity}. 
The prototypical example is the word \textit{bank} which may refer to a river \textit{bank} or to a financial institution.
Without explicit disambiguation there is no way to tell the intended meaning. Another limitation of game tags stems from the presence of \textit{synonyms} in natural languages. For example, both the tags \textit{mac} and \textit{macintosh} can be used to denote Apple Macintosh computer. If the searcher searches for \textit{mac} but only \textit{macintosh} is present in the system the search will be unsuccessful.
Yet another limitation is imposed by the general level of \textit{tag literacy}. 
Tag literacy in this sense refers to the `etiquette' of generating tags in a way that increases their social value, balancing individual needs with the needs of the community. Low level of tag literacy stems from the lack of universally accepted tagging conventions among taggers.
This may lead to the usage of specialised tags and ``nonsense" tags designed as unique markers that are shared between a group of friends or co-workers. These limitations are not exclusive to game tags. The other metadata types suffer from them as well, albeit to a lesser extent. Closed-captions and in-house catalog tags can be ambiguous and have issues with synonyms as well. Tag literacy is a less of a concern for in-house catalog tags given their more controlled nature. 

In the search studies described in Chapter \ref{chap:ecir} and \ref{chap:topicir-filter} we use standard information retrieval mechanisms to combat the tag limitations outlined above such as stemming which is a standard technique to deal with word inflexions by reducing the inflected words to their root(stem). Tag literacy was not an issue in our search studies as the queries were all standard words and the word inflexions were taken care of by stemming. In other words, the effect of low literacy tags, was not triggered by the queries in our query set. Ambiguity was also not an issue since the majority of the queries in our query set are not polysemic. And even when there is more than one meaning assigned to the queries usually the difference among them is nuanced so they can effectively be used interchangeably. Synonymy can be addressed by query expansion where the synonyms are added to the query and searched for as well. It will be interesting to extend the studies from Chapter \ref{chap:ecir} and \ref{chap:topicir-filter} to deal with synonyms in future work.

\subsection{Can the quality of user-generated tags for videos be evaluated and improved?}\label{conclusion:filter}
In Chapter \ref{chap:topicir-filter} we demonstrate that game tags are are not well suited for retrieving video fragments based on topic. This is mainly caused by the overwhelming presence of game tags which are not valid topical descriptors. In the second half of Chapter \ref{chap:topicir-filter} we evaluate the quality of the game tags as topical descriptors with the intention to detect and filter out the ones that are not referring to topics. We explore several methods for predicting the tag quality --- TF-IDF-based filtering, player reputation-based filtering, and topic models to name a few. The success of each method is judged against the evaluation dataset developed in the topical search study.

While the different methods that we studied performed with various success, the conclusion is that the
game tags can be successfully exploited for topical video search provided there is a filtering process that would reduce or eliminate the effect of the non-topical tags. Our results show that after TF-IDF-based filtering the game tags can emulate the retrieval performance of the best performing system that utilizes manually crafted metadata for search. Also, combining TF-IDF filtered game tags with the manually crafted metadata yields an improvement of retrieval performance by $18\%$. The improvement is attributed to the increased retrieval recall stemming from the game tags. An important consequence of this result is that tagging games provide a cost-effective alternative for AV collection owners that do not possess the required manpower to manually annotate their material. This is of particular importance for the audio-visual cultural heritage domain. The national audio-visual archives throughout Europe have hundreds of thousands hours of digitized audio-visual material that is not readily accessible because of lack of annotations. The audio-visual archives can now launch \textit{Waisda?}-like crowdsourcing campaigns confident that the collected tags will support tasks like visual and topical search. We come back to this point in Section \ref{con:bigger-picture}.
 
\section{Discussion and Further Research}
In this thesis we investigate the added value of the tags collected with the video-labelling game \textit{Waisda?} for video fragment retrieval. The general conclusion is that the game tags improve retrieval and on average perform better compared to the manually-crafted metadata and closed captions. This statement comes with three caveats.

First, in our setting the \textit{taggers} and the \textit{searchers} originate from the same population which results in significant terminological overlap between the \textit{keywords} used for search and the tags ascribed to the videos. We believe this to be a strong contributing factor for the success of the game tags for search in the context of our study. And while it may look like a limitation at first, in practice it is not. The terminology employed when tagging is general enough to cater to the needs of all but the expert searchers which may use more specialized (specific) terms when searching \cite{liliana}.

Second, in this work we considered only \textit{keyword-based search} thus the said improvement in retrieval is limited to this modality. At the time of writing, other search modalities such as \textit{content-based video retrieval} \cite{veltkamp2013state} are gaining more attention, however keyword-based search is still the most prominent one, especially on the Web.

Third, in this work we focussed on single domain (genre), non-fictional TV Comedy. Consequently, the obtained results are bound to this domain. However, the qualitative analysis of the tags revealed that they are usually general concepts describing visual objects and as such are not so tightly coupled with the genre of the video content they are describing. Our expectation is that the game tags will perform well for most of the non-fictional TV genres which appeal to a wider audience. It remains to be seen whether this will be the case for the fictional genres where more symbolism and abstract expressions can be expected. We hypothesise that without properly instructing the taggers or targeting a niche sub-community of experts the outcome will be less satisfactory.

\subsection{A broader perspective}\label{con:bigger-picture}
In the recent years there were advancements in many fields that are related to the scope of the work presented in this thesis. Following is a non-exhaustive list of the most relevant ones.

\subsubsection{Crowdsourcing in Cultural Heritage Domain}
In the recent years the bulk of the European cultural heritage has been undergoing a process of digitisation. A testament of that was the European Union funded project PrestoPRIME\footnote{\url{http://www.prestoprime.org/}} about digitisation and preservation of the European audio-visual heritage. The project's consortium included all major European national audio-visual archives and among many of the project's deliverables was \textit{Waisda?}. The project also delivered a digitisation and preservation framework that enables the conversion of thousands of hours of audio-visual material from analog to digital form. However, simply by being in digital form and without descriptive metadata makes the audio-visual collections only marginally more accessible than being on analog tapes. Manual annotation by professional catalogers is prohibitively expensive. According to empirical evidence it takes usually around five times the duration of a video for a cataloger to annotate it. In this thesis we have demonstrated that GWAPs such as \textit{Waisda?} provide a cost-effective alternative. It is relatively inexpensive to deploy games such as \textit{Waisda?} and after the suitable processing the collected tags can be exploited for video retrieval. This has important ramifications for the audio-visual archives. They can deploy Waisda? and collect tags for the hundreds of thousands hours of video material which are not annotated simply because it requires too much time and effort for the catalogers to do it manually. The Sound and Vision institute, the Dutch national archive, already included the tags collected with \textit{Waisda?} in the in-house workflows and is cooperating with other institutions in deploying \textit{Waisda?} in various domains. In this thesis we developed an extension module for \textit{Waisda?} which enables seamless integration of the game in any audio-visual collection workflow.

\textit{Waisda?} is not the only initiative to introduce crowdsourcing in the culture heritage domain. Public Catalogue Foundation\footnote{\url{http://www.thepcf.org.uk/}} and BBC\footnote{\url{http://www.bbc.com/}} launched \textit{Your Paintings.Tagger}\footnote{\url{http://tagger.thepcf.org.uk/}} project to annotate the British national oil painting collection (over 200,000 paintings) \cite{ellis2012your}. \textit{Your Paintings.Tagger} is an online crowdsourcing tool that enables the members of the public to tag the paintings with the ultimate goal of making the collection more searchable. Another crowdsourcing initiative is the \textit{OldWeather} project\footnote{\url{http://www.oldweather.org}} where online volunteers help scientists recover Arctic and worldwide weather observations made by ships since the mid-19th century by transcribing ships' logs. These transcriptions will contribute to climate model projections and will improve our knowledge of past environmental conditions \cite{brohan2009marine}. Several other crowdsourcing initiatives have been launched for transcribing old manuscripts \cite{causer2014many,leon2014build,chrons2011digitalkoot}: \textit{Cymru1900Wales}\footnote{\url{http://www.cymru1900wales.org/}},  \textit{Transcribe Bentham}\footnote{\url{http://www.transcribe-bentham.da.ulcc.ac.uk/td/Transcribe_Bentham}}, \textit{Scripto}\footnote{\url{http://scripto.org/why-crowdsourcing-why-scripto/}}, \textit{DigitalKoot}\footnote{\url{http://www.digitalkoot.fi/}}, and \textit{ReScript}\footnote{\url{http://www.history.ac.uk/projects/digital/ReScript}} to name a few. Many other crowdsourcing initiatives, not necessarily in the cultural heritage domain, have been launched in the recent decade. We mention some of them in section \ref{sec:topicir-filter:relatedwork}. The driving force behind each of them, \textit{Waisda?} included, is to engage the online communities to create transformative knowledge. At the time of writing, \textit{Waisda?} is the only crowdsourcing project, that we are aware of, in the domain of audio-visual cultural heritage.



\subsubsection{Computer Vision}
Thanks to the advancements made in the area of deep learning \cite{Bengio:2013:RLR:2498740.2498889,Arel:2010:RFD:1921914.1921920,Schmidhuber:deep-learning} the field of computer vision witnessed important breakthroughs in the recent years. Using vast number of high-resolution example images, the deep convolutional neural networks can be trained to classify and label scenes and objects depicted in new unseen images \cite{NIPS2012_4824,10.1109/TPAMI.2012.231}. An excellent example of this idea put into practice is the Microsoft's Project Adam \cite{project-adam}. The Adam computer vision system is hailed to be advanced enough to tell the difference, for example, between a picture of a Pembroke and a Cardigan Corgi (dog breeds), a task that is quite difficult even for a person. The Adam system is fuelled by a deep convolutional neural network which has been trained with over 14 million images split up into 22,000 categories drawn from the ImageNet database\footnote{\url{http://www.image-net.org/}}. Another computer vision task that has benefited greatly from deep learning is face recognition. The Facebook's DeepFace system is able to tell whether two previously unseen photos of faces show the same person with accuracy almost as high as that of a human regardless of variations in lighting or whether the person in the picture is directly facing the camera \cite{Taigman_2014_CVPR,DBLP:conf/cvpr/ZhangPTFB15}. The deep learning techniques have been applied to the medium of video as well. Karpathy et al. used convolutional neural networks for large-scale video classification. Deep learning has also been exploited to for human action recognition in videos \cite{DBLP:journals/corr/SimonyanZ14,Ji:2013:CNN:2412386.2412939}. In spite of all these advancements, the holy grail of computer vision, \textit{total scene understanding}, is still well outside the reach or the current state-of-the-art systems. This is even more true for the medium of video which compared to still images has the additional temporal dimension. A direct consequence of the temporal dimension and an excellent example of the complexity of scene understanding is the so-called \textit{Kuleshov effect}. It is a cognitive phenomenon by which viewers derive more meaning from the interaction of two sequential shots than from a single shot in isolation \cite{mobbs2006kuleshov}. This means that the meaning of a scene is not determined only by the scene itself but also by the scenes that preceded it. While the total scene understanding is presently unattainable for computers it is relatively easy for the average \textit{Waisda?} player. We have demonstrated that the \textit{Waisda?} tags refer ---among other things--- to the more abstract aspects of the videos such as topics.
This hints to a potential hybrid approach to video annotation that combines computer vision and a human-computation technique such as \textit{Waisda?}. The computer vision system will detect and label objects, actions, generic scene classes, etc. and the \textit{Waisda?} players will tag the more abstract aspect relating to the meaning of the scenes. In such a system there will be overlap, the computer vision system and the \textit{Waisda?} players may ascribe the same tag. This redundancy is an affirmation of the correctness of the tag. A potential drawback of the computer vision systems in general is that they need to be trained to learn new concepts. Granted the Adam system can recognize over 22,000 concepts but this is still a finite and potentially limited set. The hybrid approach could be used to overcome this problem. The computer vision system can learn new concepts from the \textit{Waisda?} tags. Indeed, the \textit{Waisda?} tags and the associated scenes can be used as training examples for the computer vision system to learn the ``unseen" concepts denoted by the tags. It remains to be seen whether the learned concepts by Adam system can be successfully used for retrieval.

\subsubsection{Multimedia Information Retrieval}
There are two major approaches to multimedia retrieval: text-based retrieval and content-based retrieval and according to Chu \cite{journals/jasis/Chu01} the former approach is the most widely used in practice. Keyword-based search, which is a type of text-based retrieval, is the dominant search modality for images and videos on the Web. In this thesis we have demonstrated that the tags collected with \textit{Waisda?} can be successfully exploited for keyword-based retrieval. This finding has important consequences for the accessibility of the videos on the Web where the size of the multimedia content has exploded exponentially in the recent years. According to the video marketing site ReelSEO\footnote{\url{http://www.reelseo.com}}, as of December 2015, every minute 300 hours of video material is being uploaded on YouTube\footnote{\url{https://www.youtube.com}}\cite{reelseo} and YouTube is just one of the major video-sharing services. \textit{Waisda?} and games alike can be used to annotate the vast number of videos on the Web and make them accessible via keyword-based search.

Although still an active research field, content-based retrieval has gained attention on the Web in the recent years. The major search engine Google\footnote{\url{https://www.google.com}} introduced a reversed image search functionality called \textit{Google Images}\footnote{\url{https://images.google.com/}}. Unlike traditional keyword-based image search, in \textit{Google Image} the searcher submits an image as query and gets similar images and web pages as result. TinEye\footnote{\url{http://tineye.com/}} is another content-based image search engine which provides reversed image search that functions similarly as \textit{Google Images}. We believe that content-based retrieval can benefit from video labelling games such as \textit{Waisda?} and their output. One way is to combine the game tags with the content-based retrieval techniques into a hybrid approach. In fact, there is already research work done on the topic of combining high-level semantics such as text with content-based modalities \cite{Popescu:2007:ODC:1282280.1282338,Yavlinsky06alarge,citeulike:4000618,Huurnink:2010:TTR:1816041.1816045,snoek2007adding}.
Another way is to exploit the \textit{Waisda?} tags to train supervised content-based retrieval methods. \textit{Waisda?} tags are associated with a timepoint and \textit{deep-link} to the image frame/video segment which they are describing. As such the tags and the associated image frames/video segments can serve as training examples.

\subsection{Future Directions}
Video labelling games are \textit{human-based computation} technique. Needless to say, anyone wishing to successfully deploy a \textit{Waisda?} instance or other games alike ought not neglect the \textit{human factor}. Even the most perfectly executed video-labelling campaign from technological standpoint will fail if there are no taggers to contribute. In other words, marketing is vital. The empirical evidence collected from both \textit{Waisda?} pilots supports this claim. On both occasions the bulk of the tags was accumulated during periods when there was active marketing campaign for promoting the game. We believe that carefully applying the marketing mechanisms for acquiring new players by targeting the right online communities is a key factor for success. We mentioned already that annotation of content from certain genres such as art which may employ abstract symbolism will require more knowledgeable individuals. Our hypothesis is that engaging the right community leads to better tags. Understanding the relationship between target marketing and user engagement on one side and the quality of the resulting tags on the other is a worthwhile direction for future research.

What makes video annotation a difficult task is the fact that video is a medium that is extremely rich in meaning. The taggers can get overwhelmed by the complex interplay of objects and events especially in the fast-paced game setting. We believe that the game design directly influences the focus and quality of the resulting tags. For example, significant portion of the \textit{Waisda?} tags refer to more noticeable aspects of the content such as moving objects in the background or in the foreground, or prominent stationary objects. This is hardly surprising, after all these are things that are easy to reach consensus on and ultimately that is the goal of the game from the perspective of the players. We hypothesise that if the underlying goals\footnote{An example goal could be tagging the occurrences of specific type of events.} of the game creators are better aligned with the game objectives from the perspective of the players that will result in increased quality of the resulting tags. Understanding how the game design and game objectives influence the quality of the resulting tags is another worthwhile direction for future research. 

We foresee that video-labelling games will be increasingly used for video annotation and eventually will become the dominant method due to their cost-effectiveness. Future research could focus on making them more effective by engaging the right communities and stimulating them to produce better tags by setting appropriate game objectives.   
That being said, the role of the catalogers and curators in future will be less about manual annotation and more about steering the annotation process --- making strategic decisions about what kind of metadata to collect, what   type of crowdsourcing campaign to launch, and which online communities to engage.



 